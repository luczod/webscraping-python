{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "imageLocation = bs.find('a', {\n",
    "    'href': 'https://pythonscraping.com'\n",
    "}).find('img')['src']\n",
    "urlretrieve(imageLocation, 'logo.jpg')\n",
    "\n",
    "# output: stores it as logo.jpg in the same directory where the script is running."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for illustrative purposes only, risks of damage to the computer through unwanted downloads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "downloadDirectory = 'downloaded'\n",
    "baseUrl: str = 'http://pythonscraping.com'\n",
    "\n",
    "\n",
    "def getAbsoluteURL(baseUrl: str, source: str):\n",
    "    if source.startswith('http://www.'):\n",
    "        url = 'http://{}'.format(source[11:])\n",
    "    elif source.startswith('http://'):\n",
    "        url = source\n",
    "    elif source.startswith('www.'):\n",
    "        url = source[4:]\n",
    "        url = 'http://{}'.format(source)\n",
    "    else:\n",
    "        url = '{}/{}'.format(baseUrl, source)\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "\n",
    "def getDownloadPath(baseUrl: str, absoluteUrl: str, downloadDirectory: str):\n",
    "    path = absoluteUrl.replace('www.', '')\n",
    "    path = path.replace(baseUrl, '')\n",
    "    path = downloadDirectory + path\n",
    "    directory = os.path.dirname(path)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return path\n",
    "\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "downloadList = bs.findAll(src=True)\n",
    "\n",
    "for download in downloadList:\n",
    "    fileUrl = getAbsoluteURL(baseUrl, download['src'])\n",
    "    if fileUrl is not None:\n",
    "        print(fileUrl)\n",
    "\n",
    "urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvFile = open('test.csv', 'w+')\n",
    "\n",
    "try:\n",
    "    writer = csv.writer(csvFile, delimiter=';')\n",
    "    writer.writerow(('number', 'number plus 2', 'number times 2'))\n",
    "    for i in range(10):\n",
    "        writer.writerow((i, i + 2, i * 2))\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Html Table in CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/'\n",
    "               'Comparison_of_text_editors')\n",
    "\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "# The main comparison table is currently the first table on the page\n",
    "table = bs.findAll('table', {'class': 'wikitable'})[0]\n",
    "rows = table.findAll('tr')\n",
    "\n",
    "csvFile = open('editors.csv', 'wt+')\n",
    "writer = csv.writer(csvFile)\n",
    "\n",
    "try:\n",
    "    for row in rows:\n",
    "        csvRow = []\n",
    "        for cell in row.findAll(['td', 'th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "            writer.writerow(csvRow)\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with MySql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1',\n",
    "                       #    unix_socket='/tmp/mysql.sock',\n",
    "                       user='root',\n",
    "                       passwd=None,\n",
    "                       db='mysql')\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE scraping')\n",
    "cur.execute('SELECT * FROM pages WHERE id=1')\n",
    "print(cur.fetchone())\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='127.0.0.1',\n",
    "    #    unix_socket='/tmp/mysql.sock',\n",
    "    user='root',\n",
    "    passwd=None,\n",
    "    db='mysql',\n",
    "    charset='utf8')  # Attention\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scraping\")\n",
    "\n",
    "random.seed(datetime.datetime.now().timestamp())\n",
    "\n",
    "\n",
    "def store(title, content):\n",
    "    cur.execute('INSERT INTO pages (title, content) VALUES '\n",
    "                '(\"%s\", \"%s\")', (title, content))\n",
    "    cur.connection.commit()  # Attention\n",
    "\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen('http://en.wikipedia.org' + articleUrl)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    content = bs.find('div', {'id': 'mw-content-text'}).find('p').get_text()\n",
    "    store(title, content)\n",
    "    return bs.find('div', {\n",
    "        'id': 'bodyContent'\n",
    "    }).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "\n",
    "try:\n",
    "    while len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\n",
    "        print(newArticle)\n",
    "        links = getLinks(newArticle)\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()  # Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "\n",
    "def sendMail(subject: str, body: str):\n",
    "    msg = MIMEText(body)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = 'from@email.com'\n",
    "    msg['To'] = 'to@email.com'\n",
    "\n",
    "    s = smtplib.SMTP('localhost')\n",
    "    s.send_message(msg)\n",
    "    s.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
