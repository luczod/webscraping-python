{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-threaded crawling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _thread\n",
    "import time\n",
    "\n",
    "\n",
    "def print_time(threadName: str, delay: int, iterations: int) -> None:\n",
    "    start = int(time.time())\n",
    "    for i in range(0, iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print(\"{} {}\".format(seconds_elapsed, threadName))\n",
    "\n",
    "\n",
    "try:\n",
    "    _thread.start_new_thread(print_time, ('Fizz', 3, 33))\n",
    "    _thread.start_new_thread(print_time, ('Buzz', 5, 20))\n",
    "    _thread.start_new_thread(print_time, ('Counter', 1, 100))\n",
    "except:\n",
    "    print('Error: unable to start thread')\n",
    "while 1:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, ResultSet\n",
    "from typing import Any\n",
    "import re\n",
    "import random\n",
    "\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "\n",
    "def get_links(thread_name: str, bs: BeautifulSoup) -> ResultSet[Any]:\n",
    "    print('Getting links in {}'.format(thread_name))\n",
    "    \n",
    "    return bs.find('div', {\n",
    "        'id': 'bodyContent'\n",
    "    }).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "\n",
    "# Define a function for the thread\n",
    "\n",
    "\n",
    "def scrape_article(thread_name, path) -> None:\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)  # attention\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    print('Scraping {} in thread {}'.format(title, thread_name))\n",
    "    links = get_links(thread_name, bs)\n",
    "    \n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\n",
    "        print(newArticle)\n",
    "        scrape_article(thread_name, newArticle)\n",
    "\n",
    "\n",
    "# Create two threads as defined below\n",
    "try:\n",
    "    _thread.start_new_thread(scrape_article, (\n",
    "        'Thread 1',\n",
    "        '/wiki/Kevin_Bacon',\n",
    "    ))\n",
    "    _thread.start_new_thread(scrape_article, (\n",
    "        'Thread 2',\n",
    "        '/wiki/Monty_Python',\n",
    "    ))\n",
    "except:\n",
    "    print('Error: unable to start thread')\n",
    "while 1:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queues and Thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import _thread\n",
    "from queue import Queue\n",
    "import time\n",
    "import pymysql\n",
    "\n",
    "\n",
    "def storage(queue: Queue) -> None:\n",
    "    conn = pymysql.connect(\n",
    "        host='127.0.0.1',\n",
    "        #    unix_socket='/tmp/mysql.sock',\n",
    "        user='root',\n",
    "        passwd='',\n",
    "        db='mysql',\n",
    "        charset='utf8')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('USE wiki_threads')\n",
    "    \n",
    "    while 1:\n",
    "        if not queue.empty():\n",
    "            article = queue.get()\n",
    "            cur.execute('SELECT * FROM pages WHERE path = %s',\n",
    "                        (article[\"path\"]))\n",
    "        if cur.rowcount == 0:\n",
    "            print(\"Storing article {}\".format(article[\"title\"]))\n",
    "            cur.execute('INSERT INTO pages (title, path) VALUES (%s, %s)',\n",
    "                        (article[\"title\"], article[\"path\"]))\n",
    "            conn.commit()\n",
    "        else:\n",
    "            print(\"Article already exists: {}\".format(article['title']))\n",
    "\n",
    "\n",
    "visited = []\n",
    "\n",
    "\n",
    "def getLinks(thread_name: str, bs: BeautifulSoup) -> list[str]:\n",
    "    print('Getting links in {}'.format(thread_name))\n",
    "    links = bs.find('div', {\n",
    "        'id': 'bodyContent'\n",
    "    }).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    \n",
    "    return [link for link in links if link not in visited]\n",
    "\n",
    "\n",
    "def scrape_article(thread_name: str, path: str, queue: Queue) -> None:\n",
    "    visited.append(path)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    print('Added {} for storage in thread {}'.format(title, thread_name))\n",
    "    queue.put({\"title\": title, \"path\": path})\n",
    "    links = getLinks(thread_name, bs)\n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\n",
    "        scrape_article(thread_name, newArticle, queue)\n",
    "\n",
    "\n",
    "queue = Queue()\n",
    "try:\n",
    "    _thread.start_new_thread(scrape_article, (\n",
    "        'Thread 1',\n",
    "        '/wiki/Kevin_Bacon',\n",
    "        queue,\n",
    "    ))\n",
    "    _thread.start_new_thread(scrape_article, (\n",
    "        'Thread 2',\n",
    "        '/wiki/Monty_Python',\n",
    "        queue,\n",
    "    ))\n",
    "    _thread.start_new_thread(storage, (queue, ))\n",
    "except:\n",
    "    print('Error: unable to start threads')\n",
    "\n",
    "while 1:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def print_time(threadName: str, delay: int, iterations: int) -> None:\n",
    "    start = int(time.time())\n",
    "    for i in range(0, iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print('{} {}'.format(seconds_elapsed, threadName))\n",
    "\n",
    "\n",
    "threading.Thread(target=print_time, args=('Fizz', 3, 33)).start()\n",
    "threading.Thread(target=print_time, args=('Buzz', 5, 20)).start()\n",
    "threading.Thread(target=print_time, args=('Counter', 1, 100)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "def crawler(url: str) -> None:\n",
    "    data = threading.local()\n",
    "    data.visited = []\n",
    "\n",
    "\n",
    "threading.Thread(target=crawler, args=('http://brookings.edu')).start()\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    if not t.isAlive():\n",
    "        t = threading.Thread(target=crawler)\n",
    "        t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler(threading.Thread):\n",
    "\n",
    "    def __init__(self):\n",
    "        # initializing the parent class\n",
    "        threading.Thread.__init__(self)\n",
    "        self.done = False\n",
    "\n",
    "    def isDone(self) -> bool:\n",
    "        return self.done\n",
    "\n",
    "    def run(self) -> None:\n",
    "        time.sleep(5)\n",
    "        self.done = True\n",
    "        raise Exception('Something bad happened!')\n",
    "\n",
    "\n",
    "t = Crawler()\n",
    "t.start()\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    if t.isDone():\n",
    "        print('Done')\n",
    "        break\n",
    "    if not t.isAlive():\n",
    "        t = Crawler()\n",
    "        t.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import time\n",
    "\n",
    "\n",
    "def print_time(threadName: str, delay: int, iterations: int) -> None:\n",
    "    start = int(time.time())\n",
    "    for i in range(0, iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print(threadName if threadName else seconds_elapsed)\n",
    "\n",
    "\n",
    "processes = []\n",
    "processes.append(Process(target=print_time, args=('Counter', 1, 100)))\n",
    "processes.append(Process(target=print_time, args=('Fizz', 3, 33)))\n",
    "processes.append(Process(target=print_time, args=('Buzz', 5, 20)))\n",
    "\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process\n",
    "import os\n",
    "import time\n",
    "\n",
    "visited = []\n",
    "\n",
    "\n",
    "def get_links(bs: BeautifulSoup) -> list[str]:\n",
    "    print('Getting links in {}'.format(os.getpid()))\n",
    "    links = bs.find('div', {\n",
    "        'id': 'bodyContent'\n",
    "    }).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "    return [link for link in links if link not in visited]\n",
    "\n",
    "\n",
    "def scrape_article(path: str) -> None:\n",
    "    visited.append(path)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    print('Scraping {} in process {}'.format(title, os.getpid()))\n",
    "    links = get_links(bs)\n",
    "\n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\n",
    "        print(newArticle)\n",
    "        scrape_article(newArticle)\n",
    "\n",
    "\n",
    "processes = []\n",
    "processes.append(Process(target=scrape_article, args=('/wiki/Kevin_Bacon', )))\n",
    "processes.append(Process(target=scrape_article, args=('/wiki/Monty_Python', )))\n",
    "\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "# page 284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from multiprocessing import Process, Queue\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def task_delegator(taskQueue: Queue, urlsQueue: Queue) -> None:\n",
    "    # Initialize com uma tarefa para cada processo\n",
    "    visited = ['/wiki/Kevin_Bacon', '/wiki/Monty_Python']\n",
    "    taskQueue.put('/wiki/Kevin_Bacon')\n",
    "    taskQueue.put('/wiki/Monty_Python')\n",
    "\n",
    "    while 1:\n",
    "        # Check for new links in urlsQueue\n",
    "        # to be processed\n",
    "        if not urlsQueue.empty():\n",
    "            links = [link for link in urlsQueue.get() if link not in visited]\n",
    "            for link in links:\n",
    "                # Add a new link to taskQueue\n",
    "                taskQueue.put(link)\n",
    "\n",
    "\n",
    "def get_links(bs: BeautifulSoup) -> list[str]:\n",
    "    links = bs.find('div', {\n",
    "        'id': 'bodyContent'\n",
    "    }).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "    return [link.attrs['href'] for link in links]\n",
    "\n",
    "\n",
    "def scrape_article(taskQueue: Queue, urlsQueue: Queue) -> None:\n",
    "    while 1:\n",
    "        while taskQueue.empty():\n",
    "            # Sleep for 100 ms while waiting for task queue\n",
    "            # This must be rare\n",
    "            time.sleep(.1)\n",
    "    path = taskQueue.get()\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    print('Scraping {} in process {}'.format(title, os.getpid()))\n",
    "    links = get_links(bs)\n",
    "    # Envia ao código de delegação para processamento\n",
    "    urlsQueue.put(links)\n",
    "\n",
    "\n",
    "processes = []\n",
    "taskQueue = Queue()\n",
    "urlsQueue = Queue()\n",
    "processes.append(Process(target=task_delegator, args=(\n",
    "    taskQueue,\n",
    "    urlsQueue,\n",
    ")))\n",
    "\n",
    "processes.append(Process(target=scrape_article, args=(\n",
    "    taskQueue,\n",
    "    urlsQueue,\n",
    ")))\n",
    "\n",
    "processes.append(Process(target=scrape_article, args=(\n",
    "    taskQueue,\n",
    "    urlsQueue,\n",
    ")))\n",
    "\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "# page 286"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
