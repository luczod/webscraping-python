{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-threaded crawling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _thread\n",
    "import time\n",
    "\n",
    "\n",
    "def print_time(threadName, delay, iterations):\n",
    "    start = int(time.time())\n",
    "    for i in range(0, iterations):\n",
    "        time.sleep(delay)\n",
    "        seconds_elapsed = str(int(time.time()) - start)\n",
    "        print(\"{} {}\".format(seconds_elapsed, threadName))\n",
    "\n",
    "\n",
    "try:\n",
    "    _thread.start_new_thread(print_time, ('Fizz', 3, 33))\n",
    "    _thread.start_new_thread(print_time, ('Buzz', 5, 20))\n",
    "    _thread.start_new_thread(print_time, ('Counter', 1, 100))\n",
    "except:\n",
    "    print('Error: unable to start thread')\n",
    "while 1:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "\n",
    "def get_links(thread_name, bs):\n",
    "    print('Getting links in {}'.format(thread_name))\n",
    "    return bs.find('div', {\n",
    "        'id': 'bodyContent'\n",
    "    }).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "\n",
    "# Define a function for the thread\n",
    "\n",
    "\n",
    "def scrape_article(thread_name, path):\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(path))\n",
    "    time.sleep(5)  # attention\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    print('Scraping {} in thread {}'.format(title, thread_name))\n",
    "    links = get_links(thread_name, bs)\n",
    "    if len(links) > 0:\n",
    "        newArticle = links[random.randint(0, len(links) - 1)].attrs['href']\n",
    "        print(newArticle)\n",
    "        scrape_article(thread_name, newArticle)\n",
    "\n",
    "\n",
    "# Create two threads as defined below\n",
    "try:\n",
    "    _thread.start_new_thread(scrape_article, (\n",
    "        'Thread 1',\n",
    "        '/wiki/Kevin_Bacon',\n",
    "    ))\n",
    "    _thread.start_new_thread(scrape_article, (\n",
    "        'Thread 2',\n",
    "        '/wiki/Monty_Python',\n",
    "    ))\n",
    "except:\n",
    "    print('Error: unable to start thread')\n",
    "while 1:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
