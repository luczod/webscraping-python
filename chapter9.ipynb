{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing in natural languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pprint\n",
    "\n",
    "\n",
    "def cleanSentence(sentence: str) -> str:\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [\n",
    "        word.strip(string.punctuation + string.whitespace) for word in sentence\n",
    "    ]\n",
    "    sentence = [\n",
    "        word for word in sentence\n",
    "        if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')\n",
    "    ]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def cleanInput(content: str) -> list[str]:\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "    content = bytes(content, \"UTF-8\")\n",
    "    content = content.decode(\"ascii\", \"ignore\")\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "def getNgramsFromSentence(content: str, n: int) -> list[str]:\n",
    "    output = []\n",
    "    for i in range(len(content) - n + 1):\n",
    "        output.append(content[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def getNgrams(content: str, n: int) -> Counter:\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [\n",
    "            ' '.join(ngram) for ngram in getNgramsFromSentence(sentence, 2)\n",
    "        ]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return (ngrams)\n",
    "\n",
    "\n",
    "content = str(\n",
    "    urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(),\n",
    "    'utf-8')\n",
    "\n",
    "ngrams = getNgrams(content, 2)\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(ngrams)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a filter for relevants words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pprint\n",
    "\n",
    "\n",
    "def isCommon(ngram: list[str]) -> bool:\n",
    "    commonWords = [\n",
    "        'THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT',\n",
    "        'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS',\n",
    "        'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE',\n",
    "        'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD',\n",
    "        'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE',\n",
    "        'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH',\n",
    "        'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE',\n",
    "        'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW',\n",
    "        'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK',\n",
    "        'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN',\n",
    "        'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL'\n",
    "    ]\n",
    "\n",
    "    for word in ngram:\n",
    "        if word in commonWords:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def cleanSentence(sentence: list[str]) -> list[str]:\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [\n",
    "        word.strip(string.punctuation + string.whitespace) for word in sentence\n",
    "    ]\n",
    "    sentence = [\n",
    "        word for word in sentence\n",
    "        if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')\n",
    "    ]\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def cleanInput(content: list[list[str]]) -> list[list[str]]:\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "    content = bytes(content, \"UTF-8\")\n",
    "    content = content.decode(\"ascii\", \"ignore\")\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "def getNgramsFromSentence(content: list[list[str]], n: int) -> list[str]:\n",
    "    output = []\n",
    "    for i in range(len(content) - n + 1):\n",
    "        output.append(content[i:i + n])\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def getNgrams(content: list[list[str]], n: int) -> Counter:\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [\n",
    "            ' '.join(ngram) for ngram in getNgramsFromSentence(sentence, 2)\n",
    "        ]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return (ngrams)\n",
    "\n",
    "\n",
    "content = str(\n",
    "    urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(),\n",
    "    'utf-8')\n",
    "\n",
    "ngrams = getNgrams(content, 2)\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(ngrams)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from typing import Literal\n",
    "from random import randint\n",
    "\n",
    "def wordListSum(wordList: dict[str,dict[str, int]]) -> int:\n",
    "    if not isinstance(wordList, dict):\n",
    "        raise ValueError(\"Input must be a dictionary.\")\n",
    "    \n",
    "    total_sum = 0\n",
    "    for values in wordList.values():\n",
    "        if isinstance(values, dict):\n",
    "            total_sum += sum(values.values())\n",
    "        else:\n",
    "            raise ValueError(\"Inner values must be dictionaries.\")\n",
    "    return total_sum\n",
    "\n",
    "\n",
    "def retrieveRandomWord(wordList: dict[str, int]) -> (str | None):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "\n",
    "def buildWordDict(text: str) -> dict:\n",
    "    # Remove line breaks and quotation marks\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\"', '')\n",
    "\n",
    "    # Ensures that punctuation marks are treated as proper \"words\",\n",
    "    # so that they are included in the Markov chain\n",
    "    punctuation = [',', '.', ';', ':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, ' {} '.format(symbol))\n",
    "    words = text.split(' ')\n",
    "    # Filter empty words\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    wordDict: dict[str,dict[str, int]] = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i - 1] not in wordDict:\n",
    "            # Create a new dictionary for this word\n",
    "            wordDict[words[i - 1]] = {}\n",
    "        if words[i] not in wordDict[words[i - 1]]:\n",
    "            wordDict[words[i - 1]][words[i]] = 0\n",
    "        wordDict[words[i - 1]][words[i]] += 1\n",
    "\n",
    "    return wordDict\n",
    "\n",
    "\n",
    "# Generates a Markov chain of size 100\n",
    "text = str(\n",
    "    urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(),\n",
    "    'utf-8')\n",
    "wordDict = buildWordDict(text)\n",
    "\n",
    "length = 100\n",
    "chain = ['I']\n",
    "for i in range(0, length):\n",
    "    newWord = retrieveRandomWord(wordDict[chain[-1]])\n",
    "    chain.append(newWord)\n",
    "    \n",
    "print(' '.join(chain))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Six Degrees (breadth-first search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='127.0.0.1',\n",
    "    #    unix_socket='/tmp/mysql.sock',\n",
    "    user='',\n",
    "    passwd='',\n",
    "    db='mysql',\n",
    "    charset='utf8')\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE wikipedia')\n",
    "\n",
    "\n",
    "def getUrl(pageId: str):\n",
    "    cur.execute('SELECT url FROM pages WHERE id = %s', (int(pageId)))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def getLinks(fromPageId: str):\n",
    "    cur.execute('SELECT toPageId FROM links WHERE fromPageId = %s',\n",
    "                (int(fromPageId)))\n",
    "    if cur.rowcount == 0:\n",
    "        return []\n",
    "    return [x[0] for x in cur.fetchall()]\n",
    "\n",
    "\n",
    "def searchBreadth(targetPageId: str, paths=[[1]]):\n",
    "    # recursive\n",
    "    newPaths = []\n",
    "    for path in paths:\n",
    "        links = getLinks(path[-1])\n",
    "        for link in links:\n",
    "            if link == targetPageId:\n",
    "                return path + [link]\n",
    "            else:\n",
    "                newPaths.append(path + [link])\n",
    "        return searchBreadth(targetPageId, newPaths)\n",
    "\n",
    "\n",
    "nodes = getLinks(1)\n",
    "targetPageId = 28624\n",
    "pageIds = searchBreadth(targetPageId)\n",
    "\n",
    "for pageId in pageIds:\n",
    "    print(getUrl(pageId))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import Text\n",
    "\n",
    "tokens = word_tokenize('Here is some not very interesting text')\n",
    "text = Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import text6\n",
    "from nltk import ngrams\n",
    "\n",
    "fourgrams = ngrams(text6, 4)\n",
    "\n",
    "for fourgram in fourgrams:\n",
    "    if fourgram[0] == 'coconut':\n",
    "        print(fourgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "\"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = word_tokenize('Strange women lying in ponds distributing swords'\n",
    "                     'is no basis for a system of government.')\n",
    "\n",
    "pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "sentences = sent_tokenize(\n",
    "    'Google is one of the best companies in the world.'\n",
    "    ' I constantly google myself to see what I\\'m up to.')\n",
    "\n",
    "nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "for sentence in sentences:\n",
    "    if 'google' in sentence.lower():\n",
    "        taggedWords = pos_tag(word_tokenize(sentence))\n",
    "        for word in taggedWords:\n",
    "            if word[0].lower() == 'google' and word[1] in nouns:\n",
    "                print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
